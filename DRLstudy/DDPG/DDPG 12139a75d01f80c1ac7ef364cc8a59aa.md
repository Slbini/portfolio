# DDPG

soft_tau 0.001, 0.005

batchsize 64,128,256

max episode 500,1000
![image 1](https://github.com/user-attachments/assets/106d366d-2508-4e01-9543-4c79395b6976)


이건 tau=0.005, batch size 64, 
![image 2](https://github.com/user-attachments/assets/29952bbe-d048-4e7e-a238-c1bb6946a9dc)



 soft update 할 때 tau를 0.001로 했을 때의 reward 그래프

- 훈련 시간이 엄청 짧고 성능이 안 좋음

- tau가 너무 작으면 target이 새롭게 업데이트가 안 돼서 성능도 떨어지나 봄
-> 그래서 그냥 다시 0.005로 바꿈
![image 3](https://github.com/user-attachments/assets/3263e882-f5db-46f1-9fd1-884385c0825d)



---

---

### hyperparameter

max_episode = 1000   # Number of episodes for agent training
evaluate_period = 5   # Episode interval for policy evaluation
evaluate_num = 10   # Number of episodes for agent evaluation

actor_initial_lr = 1e-4   # Initial learning rate for Actor
actor_final_lr = 1e-5   # Final learning rate for Actor

critic_initial_lr = 1e-3   # Initial learning rate for Critic
critic_final_lr = 1e-4   # Final learning rate for Critic

gamma = 0.99  # Reward discount rate

### Replay Buffer

buffer_size = 100000   # Size of the replay buffer
batch_size = 64   # Size of the mini-batch

---

→ 지금까지 성능 제일 잘 나옴

- episode길이가 1000정도로 충분히 길어야하는 듯 (500은 해보니까 성능이 잘 안 나옴)
- hard update 보다 soft update 성능이 훨씬 좋다.
- soft update 할 때 tau를 0.001로 하면 훈련 시간도 얼마 안 걸리고 성능도 안 나옴 reward 100정도 나옴

→ 0.005로 했더니 reward가 700가까이, 혹은 그보다 높게 결과가 나옴.

- batch_size를 64→128했더니 성능이 급 상승함! 그래서 256 해봐야겠음

### 다른 건 그대로 유지하고 batch size = 128

- batch_size = 64 -> 128 ->256 해보기 시도
![image 4](https://github.com/user-attachments/assets/fe1e22ae-2b27-44ab-aad9-3780464df2e3)
![image 5](https://github.com/user-attachments/assets/9ebd8fe5-a25a-44c3-8ffa-ffe95194ecbf)

-> 64에서 128로 바꾸니까  아까보다도 더 빨리 reward가 상승했다. 

근데 뒤로 가면 왜 떨어지지

### Batch size = 256

batch size를 늘릴 수록 속도는 더 느려지는 듯

돌리는 중 ㅜㅜ
![image 5](https://github.com/user-attachments/assets/ec77db3b-e176-4a1f-a94b-1664af2cedab)

심지어 마지막에도 36분이나 걸림

총 47+96+36분 ㅜㅜ

아주 하루종일 돌렸넹
![image 6](https://github.com/user-attachments/assets/9e2e65ef-9609-4810-92b2-44a824a316b3)


개 오래걸렸는데 좋당 ㅎㅎㅎㅎ헤힣

### 질문

Q: 근데 왜 batch_size를 늘리니까 시간이 늘어나는 거는 그렇다치고 training time이 점점 늘어나는거지????
Q: 똑같은 코드를 돌리는데 터미널에서는 60.29(min)이 찍히고 여기에서는 23.25(min)이 찍힘. 왜지?

Q.왜 reward가 최대치를 찍고 다시 하락하는지? 이럴 땐 그냥 에피소드 개수를 줄여줘야하는건지?!?

### 갑자기 이것저것 바꿔서 해본거 (망)

gamma = 0.99 → 0.999

lr: 

actor_initial_lr = 1e-3   # start learning rate of Actor
actor_final_lr   = 1e-4   # final learning rate of Actor

critic_initial_lr = 1e-4   # start learning rate of Critic
critic_final_lr   = 1e-5   # final learning rate of Critic

로 변경

buffer_size = 100000   
batch_size  = 128

soft_tau = 0.005


![image 7](https://github.com/user-attachments/assets/16bb9ae8-3046-4e7e-ac5b-58da86f3cd34)

오… 노답..
![image](https://github.com/user-attachments/assets/01362b24-4c87-40f0-953d-14ba62435939)



ㅋㅋㅋㅋㅋ…
