{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4b6d6c",
   "metadata": {},
   "source": [
    "### **Deep Reinforcement Learning(DRL)** Code\n",
    "\n",
    "---\n",
    "\n",
    "- DRL aims to **solve MDP**(Markov Decision Process) problems. That is, DRL aims to **find an optimal policy**.\n",
    "- In this notebook, we aims to implement the following DRL algorithm : `DDQG`\n",
    "- As an environment in which agents will interact, we will use [OpenAI Gymnasium library](https://gymnasium.farama.org/)\n",
    "\n",
    "Code Reference\n",
    "- https://github.com/ghliu/pytorch-ddpg/blob/master/ddpg.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7d5aa",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium\n",
    "#!pip install gymnasium[mujoco]\n",
    "#!pip install opencv-python==4.8.0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "from IPython.display import Image\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f146b4",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler (linear)\n",
    "# decay learning rate linearly from 'initial_lr' to 'final_lr'\n",
    "def linear_schedule(episode, max_episode, initial_lr, final_lr):\n",
    "    start, end = initial_lr, final_lr\n",
    "    if episode < max_episode:\n",
    "        return (start*(max_episode-episode) + end*episode) / max_episode\n",
    "    else:\n",
    "        return end\n",
    "#입실론 줄일 때도 사용 # 갈 수록 0으로 가깝게 #여기서는 linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the experiment results (rewards)\n",
    "# given the list of rewards, plot the highest, lowest, and mean reward graph.\n",
    "def plot(rewards, title:str, save_path=None):  \n",
    "      \n",
    "    plt.figure(figsize=[4,2], dpi=300)\n",
    "    plt.title(title , fontsize=9)\n",
    "    # plot reward\n",
    "    high_rewards= np.max(rewards , axis= 0)\n",
    "    low_rewards= np.min(rewards , axis= 0)\n",
    "    mean_rewards= np.mean(rewards , axis= 0)\n",
    "    \n",
    "    plt.xlabel('Episodes', fontsize=7)\n",
    "    plt.ylabel('Total Rewards', fontsize=7)\n",
    "    plt.xticks(fontsize=5)\n",
    "    plt.yticks(fontsize=5)\n",
    "    plt.grid(linewidth=.1)\n",
    "\n",
    "    x= np.arange(1, len(rewards[0])+1)\n",
    "    plt.plot(x, high_rewards, 'b-', linewidth=.1, alpha=0.2)\n",
    "    plt.plot(x, low_rewards, 'b-', linewidth=.1, alpha=0.2)\n",
    "    plt.plot(x, mean_rewards, 'b-', linewidth=.2)\n",
    "    \n",
    "    if save_path!=None:\n",
    "        plt.savefig(save_path, format='png')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GIF that shows how agent interacts with environment (play 1 episode)\n",
    "# given the trained agent and environment, the agent interact with the environment and save into a GIF file \n",
    "def play_and_save(env, agent, name='', seed=None):\n",
    "    \n",
    "    render_images = []\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    image_array = env.render()\n",
    "    render_images.append(PIL.Image.fromarray(image_array))\n",
    "\n",
    "    terminated, truncated = False, False\n",
    "    agent.behavior_Critic = agent.behavior_Critic.to('cpu') ###\n",
    "    \n",
    "    # episode start\n",
    "    while not terminated and not truncated:\n",
    "        action = agent.get_action(state).detach().numpy() ###\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        image_array = env.render()\n",
    "        render_images.append(PIL.Image.fromarray(image_array))\n",
    "    # episode finished\n",
    "    filename = 'play_' + name + '.gif'\n",
    "\n",
    "    # create and save GIF\n",
    "    render_images[0].save(filename, save_all=True, optimize=False, append_images=render_images[1:], duration=500, loop=0)\n",
    "\n",
    "    print(f'Episode Length : {len(render_images)-1}')\n",
    "    print(f'Total rewards : {total_reward}')\n",
    "    print('GIF is made successfully!')\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539f89b",
   "metadata": {},
   "source": [
    "## Objective funtion of `DDPG`\n",
    "> **Actor** ($\\theta$) : $\\nabla J(\\theta) \\approx \\frac{1}{|B|}\\sum_{i \\in B}\n",
    " \\nabla_{a}Q_{\\phi}(s_i,a)\\,\\big|_{a=\\mu_{\\theta}(s_i)}\n",
    " \\, \\nabla_{\\theta} \\mu_{\\theta}(s_i)$\n",
    " <br><br>\n",
    " **Critic** ($\\phi$) : $L(\\phi)= \\, \\frac{1}{|B|}\\sum_{i \\in B}[$\n",
    "<font color=blue>$r_{i+1} + \\gamma\\,  \\hat{Q}_{\\hat{\\phi}}(s_{i+1},\\hat{\\mu}_{\\hat{\\theta}}(s_{i+1}))$</font>\n",
    " $ - \\, Q_{\\phi}(s_i,a_i)]^2$\n",
    " <br>\n",
    " where $B$ is the sampled mini-batch from the replay buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise generator for 'exploration' of deterministic policy\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess): #noise 종류 중 하나\n",
    "    #TD3에서는 이거 안 쓰고 Gaussian noise 사용\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "        #물리적인 특성 반영해서 noise 선택\n",
    "        #temporal하게 correlated 된 노이스 생성\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use OrnsteinUhlenbeckProcess\n",
    "nb_actions = 3 # number of noises to be generated\n",
    "ou_theta = 0.15 # 'noise theta'\n",
    "ou_mu = 0.0 # 'noise mu'\n",
    "ou_sigma = 0.2 # 'noise sigma'\n",
    "\n",
    "rp = OrnsteinUhlenbeckProcess(size=nb_actions, theta=ou_theta, mu=ou_mu, sigma=ou_sigma)\n",
    "rp.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network structure of Actor and Critic\n",
    "#강화학습은 network 영향인 경우 많이 없어서 잘 안 건들임...\n",
    "#다른 hyperparameter에 비해 영향이 적음\n",
    "class Actor_net(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_high):\n",
    "        super().__init__()\n",
    "        hidden_space1 = 400\n",
    "        hidden_space2 = 300\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.Tanh() )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_dim),\n",
    "            nn.Tanh() ) # use 'Tanh' in the output layer for the bounded policy\n",
    "        #안 쓰면 1000, 20000등의 이상한 값이 나오지 않게 bounded 되게 해줌\n",
    "        #원래 이거 빼고 relu 많이 사용\n",
    "        self.action_high = torch.nn.Parameter(torch.FloatTensor(action_high), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.shared_net(x.float())\n",
    "        action = self.output_layer(x)\n",
    "\n",
    "        return self.action_high*action #(-1,1)*3=(-3,3) -> 터무니 없는 action 프린트 하지 않음\n",
    "\n",
    "class Critic_net(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        hidden_space1 = 16\n",
    "        hidden_space2 = 32\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim, hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.Tanh() )\n",
    "        \n",
    "        self.output_layer= nn.Linear(hidden_space2, 1)\n",
    "        \n",
    "    def forward(self, s, a):\n",
    "        x = torch.concat((s, a), dim =1 )\n",
    "        x = self.shared_net(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent\n",
    "### 보통 5개 정도로... \n",
    "# Agent that will be interact with environment and trained\n",
    "class DDPG:\n",
    "    def __init__(self, state_dim, action_dim, action_high, gamma, device):\n",
    "        self.behavior_Actor = Actor_net(state_dim, action_dim, action_high)\n",
    "        self.behavior_Critic = Critic_net(state_dim, action_dim)\n",
    "        self.target_Actor = Actor_net(state_dim, action_dim, action_high)\n",
    "        self.target_Critic = Critic_net(state_dim, action_dim)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.state_dim  =  state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        \n",
    "        #noise 추가\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=action_dim, theta=0.15, mu=0.0, sigma=0.2)\n",
    "    \n",
    "    # get action from the actor\n",
    "    # add noise only when training \n",
    "    def get_action(self, state, test=False):\n",
    "        #그냥 단순히 actor에 state넣어주어서 action 얻음\n",
    "        action = self.behavior_Actor.to('cpu')( torch.FloatTensor(np.array([state])))[0]\n",
    "        if test == False:\n",
    "            action += torch.FloatTensor(self.random_process.sample()) # add noise for exploration\n",
    "\n",
    "        return action\n",
    "    \n",
    "    ###젤중요한 update부분!!!\n",
    "    # update actor and critic 1-step\n",
    "    def update(self, actor_optimizer, critic_optimizer, buffer, batch_size):\n",
    "        #buffer에서 transition sampling (s,a,r,s',done)\n",
    "        state_arr, action_arr, reward_arr, next_state_arr, done_arr = buffer.sampling(batch_size,self.device)\n",
    "\n",
    "        # update Critic\n",
    "        # if True= 1, (1-done_arr)=0 => Q값이 0 =>학습시켜서 안 좋은 상태임을 학습\n",
    "        #done에는 truncated말고 terminated 값을 저장해야함\n",
    "        target = reward_arr + self.gamma*self.target_Critic.to(self.device)(next_state_arr, self.target_Actor.to(self.device)(next_state_arr))*(1-done_arr)\n",
    "        predict = self.behavior_Critic.to(self.device)(state_arr, action_arr)\n",
    "        td_error = target.detach() -  predict\n",
    "        critic_loss = torch.mean(td_error**2)\n",
    "        \n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step() \n",
    "        \n",
    "        # update Actor\n",
    "        # Q-function을 구해야 함\n",
    "        # Q(s,파이(s)=a) -> 세타(지금 policy)로 미분\n",
    "        #지금 행동한 policy 평가기 때문에 지금 얻은 action을 얻음\n",
    "        action = self.behavior_Actor.to(self.device)(state_arr)\n",
    "        predict_actor = self.behavior_Critic.to(self.device)(state_arr, action.clone())\n",
    "        actor_loss  = torch.mean((-1)*predict_actor) #원래 maximize하는 거 였는데 이 코드가 minimize하는 거라서 -1곱해줌\n",
    "                                                            \n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()   \n",
    "    \n",
    "    # update target network\n",
    "    # we can choose 'soft update' or 'hard update'\n",
    "    def update_target(self, soft_tau=None):\n",
    "        if soft_tau == None:\n",
    "            self.target_Actor.load_state_dict(self.behavior_Actor.state_dict())\n",
    "            self.target_Critic.load_state_dict(self.behavior_Critic.state_dict())\n",
    "        \n",
    "        elif soft_tau:\n",
    "            for behavior, target in zip(self.behavior_Actor.parameters(), self.target_Actor.parameters()):\n",
    "                target.data.copy_(soft_tau * behavior.data + (1.0 - soft_tau) * target.data)\n",
    "            for behavior, target in zip(self.behavior_Critic.parameters(), self.target_Critic.parameters()):\n",
    "                target.data.copy_(soft_tau * behavior.data + (1.0 - soft_tau) * target.data)\n",
    "    \n",
    "    # train agent \n",
    "    def train(self, env, max_episode, evaluate_period, evaluate_num,\n",
    "              actor_initial_lr, actor_final_lr, critic_initial_lr, critic_final_lr,\n",
    "              update_period, target_update_period, buffer_size, batch_size, soft_tau):\n",
    "        start = time.time()\n",
    "        reward_list = [] # evaluation result(reward) will be inserted during the training\n",
    "        \n",
    "        replay_buffer = ReplayBuffer(capacity=buffer_size)\n",
    "        \n",
    "        actor_optimizer = torch.optim.Adam(self.behavior_Actor.parameters(), lr=actor_initial_lr)\n",
    "        critic_optimizer = torch.optim.Adam(self.behavior_Critic.parameters(), lr=critic_initial_lr)\n",
    "        self.update_target()\n",
    "        \n",
    "        for episode in range(max_episode):\n",
    "            # new episode start\n",
    "            done = False\n",
    "            episode_length = 0\n",
    "            \n",
    "            actor_lr      = linear_schedule(episode, max_episode, actor_initial_lr, actor_final_lr)\n",
    "            actor_optimizer.learning_rate = actor_lr\n",
    "            critic_lr      = linear_schedule(episode, max_episode, critic_initial_lr, critic_final_lr)\n",
    "            critic_optimizer.learning_rate = critic_lr\n",
    "            \n",
    "            state, info = env.reset()\n",
    "            \n",
    "            # interact with environment and train\n",
    "            while not done:\n",
    "                action = self.get_action(state).detach().numpy()\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                episode_length += 1\n",
    "                \n",
    "                replay_buffer.store([state, action, [reward], next_state, [terminated]])\n",
    "                \n",
    "                # update behavior\n",
    "                if replay_buffer.size() >= batch_size and episode_length%update_period == 0:\n",
    "                    self.update(actor_optimizer, critic_optimizer, replay_buffer, batch_size)\n",
    "                # update target\n",
    "                if replay_buffer.size() >= batch_size and soft_tau == None and episode_length%target_update_period == 0:\n",
    "                    self.update_target()\n",
    "                elif replay_buffer.size() >= batch_size and soft_tau :\n",
    "                    self.update_target(soft_tau)\n",
    "\n",
    "                state = next_state\n",
    "                                \n",
    "            # episode finished and evaluate the current policy\n",
    "            if (episode+1)%evaluate_period == 0 :\n",
    "                reward = self.test(env, evaluate_num)\n",
    "                reward_list.append(reward)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(f'Training time : {(end-start)/60:.2f}(min)')\n",
    "\n",
    "        return reward_list\n",
    "    \n",
    "    # evaluate current policy\n",
    "    # return average reward value over the several episodes\n",
    "    def test(self, env, evaluate_num=10):\n",
    "\n",
    "        reward_list = []\n",
    "\n",
    "        for episode in range(evaluate_num):\n",
    "            # new episode start\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            state, info = env.reset()\n",
    "            while not done:\n",
    "                action = self.get_action(state, test=True).detach().numpy()\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            # episode finished\n",
    "            reward_list.append(episode_reward)\n",
    "\n",
    "        return np.mean(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a377c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer for experience replay\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sampling(self, batch_size, device):\n",
    "        experience_samples = random.sample(self.buffer, batch_size)\n",
    "        state_arr, action_arr, reward_arr, next_state_arr, done_arr = map(np.asarray, zip(*experience_samples))\n",
    "\n",
    "        state_arr      = torch.FloatTensor(state_arr).to(device)\n",
    "        action_arr       = torch.FloatTensor(action_arr).to(device)\n",
    "        reward_arr       = torch.FloatTensor(reward_arr).to(device)\n",
    "        next_state_arr = torch.FloatTensor(next_state_arr).to(device)\n",
    "        done_arr         = torch.FloatTensor(done_arr).to(device)\n",
    "\n",
    "        return state_arr, action_arr, reward_arr, next_state_arr, done_arr\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9771037",
   "metadata": {},
   "source": [
    "## **Train** Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter \n",
    "max_episode = 1000   # the number of episodes that agent will be trained\n",
    "evaluate_period = 5   # episode period that agent's policy will be evaluated\n",
    "evaluate_num    = 10   # the number of episodes that agent will be evaluated\n",
    "\n",
    "actor_initial_lr = 1e-4   # start learning rate of Actor\n",
    "actor_final_lr   = 1e-5   # final learning rate of Actor  \n",
    "critic_initial_lr = 1e-3   # start learning rate of Critic\n",
    "critic_final_lr   = 1e-4   # final learning rate of Critic  \n",
    "gamma = 0.99  # gamma : reward discount rate\n",
    "\n",
    "# Replay Buffer\n",
    "buffer_size = 100000   # size of the replay buffer \n",
    "batch_size  = 256   # size of the mini-batch\n",
    "\n",
    "# Target network\n",
    "target_update_period = 20 # hard update (if use soft update, set this value to any positive integer such as '20')\n",
    "soft_tau = 0.005 # soft update (if use hard update, set this value to 'None')\n",
    "update_period = 1 # step period that agent will be trained\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "repeat = 3 # repeat same experiment for the reliable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : DDPG\n",
    "reward_list =[]\n",
    "for i in range(repeat):\n",
    "    # control randomness for reproducibility\n",
    "    seed = 100*(i+1)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = gym.make('InvertedPendulum-v5', render_mode='rgb_array')\n",
    "    agent = DDPG(env.observation_space.shape[0], env.action_space.shape[0],env.action_space.high, gamma, device)\n",
    "    reward = agent.train( env, max_episode, evaluate_period, evaluate_num,\n",
    "                         actor_initial_lr, actor_final_lr, critic_initial_lr, critic_final_lr,\n",
    "                        update_period, target_update_period, buffer_size, batch_size, soft_tau )\n",
    "\n",
    "    reward_list.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9aa57e",
   "metadata": {},
   "source": [
    "## **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f21e1",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save the result\n",
    "save_folder = '/home/work/DLmath/Seulbin/DRLstudy/DDPG/result'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'DDPG6.png'\n",
    "save_path  = os.path.join(save_folder, save_file)\n",
    "\n",
    "plot(reward_list, 'DDPG', save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec62b9",
   "metadata": {},
   "source": [
    "### Test the trained agent and save it into a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(140, 90))\n",
    "display.start()\n",
    "\n",
    "env = gym.make('InvertedPendulum-v5', render_mode='rgb_array')\n",
    "play = play_and_save(env, agent, 'DDPG', seed=8)\n",
    "\n",
    "display.stop()\n",
    "Image(open(play,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c73b68",
   "metadata": {},
   "source": [
    "### comment\n",
    "\n",
    "*   soft update로 하니까 learning time이 길어짐\n",
    "*   에피소드 길이 500 -> 1000\n",
    "* soft_tau: 0.005 -> 0.001 / reward가 700이 넘었었는데 왜 100도 안 되게 바꼈지??? 그리고 훈련 속도가 너무 빨라짐 50분 -> 5분 뭐징\n",
    "-> tau가 너무 작으면 target이 새롭게 업데이트가 안 돼서 성능도 떨어지나 봄 \n",
    "-> 그래서 그냥 다시 0.005로 바꿈\n",
    "* batch_size  = 64 -> 128 ->256 해보기\n",
    "-> 64에서 128로 바꾸니까 성능이 갑자기 엄청 좋아졌다. 256 해보기\n",
    "\n",
    "* 다음 번에는 hyperparameter 변경해가면서 실험할 때, hyperparameter를 얼마로 맞춰놨는지와 결과를 따로 기록해야겠다고 생각 ㅜㅜ \n",
    "\n",
    "Q: 근데 왜 batch_size를 늘리니까 시간이 늘어나는 거는 그렇다치고 training time이 점점 늘어나는거지????\n",
    "Q: 똑같은 코드를 돌리는데 터미널에서는 60.29(min)이 찍히고 여기에서는 23.25(min)이 찍힘. 왜지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3c4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
