{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbbed60d",
   "metadata": {},
   "source": [
    "### **Deep Reinforcement Learning(DRL)** Code\n",
    "\n",
    "---\n",
    "\n",
    "- DRL aims to **solve MDP**(Markov Decision Process) problems. That is, DRL aims to **find an optimal policy**.\n",
    "- In this notebook, we aims to implement the following DRL algorithm : `REINFORCE`, `REINFORCE with baseline`\n",
    "- As an environment in which agents will interact, we will use [OpenAI Gymnasium library](https://gymnasium.farama.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a441ef",
   "metadata": {},
   "source": [
    "## Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium\n",
    "#!pip install gymnasium[mujoco]\n",
    "#!pip install opencv-python==4.8.0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04931c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "from IPython.display import Image\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9fa2d8",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate/epsilon scheduler (linear)\n",
    "# decay linearly from 'initial' to 'final'\n",
    "def linear_schedule(episode, max_episode, initial, final):\n",
    "    start, end = initial, final\n",
    "    if episode < max_episode:\n",
    "        return (start*(max_episode-episode) + end*episode) / max_episode\n",
    "    else:\n",
    "        return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the experiment results (rewards)\n",
    "# given the list of rewards, plot the highest, lowest, and mean reward graph.\n",
    "def plot(rewards, title:str, save_path=None):  \n",
    "      \n",
    "    plt.figure(figsize=[4,2], dpi=300)\n",
    "    plt.title(title , fontsize=9)\n",
    "    # plot reward\n",
    "    high_rewards= np.max(rewards , axis= 0)\n",
    "    low_rewards= np.min(rewards , axis= 0)\n",
    "    mean_rewards= np.mean(rewards , axis= 0)\n",
    "    \n",
    "    plt.xlabel('Episodes', fontsize=7)\n",
    "    plt.ylabel('Total Rewards', fontsize=7)\n",
    "    plt.xticks(fontsize=5)\n",
    "    plt.yticks(fontsize=5)\n",
    "    plt.grid(linewidth=.1)\n",
    "\n",
    "    x= np.arange(1, len(rewards[0])+1)\n",
    "    plt.plot(x, high_rewards, 'b-', linewidth=.1, alpha=0.2)\n",
    "    plt.plot(x, low_rewards, 'b-', linewidth=.1, alpha=0.2)\n",
    "    plt.plot(x, mean_rewards, 'b-', linewidth=.2)\n",
    "    \n",
    "    if save_path!=None:\n",
    "        plt.savefig(save_path, format='png')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc46508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GIF that shows how agent interacts with environment (play 1 episode)\n",
    "# given the trained agent and environment, the agent interact with the environment and save into a GIF file \n",
    "def play_and_save(env, agent, name='', seed=None):\n",
    "    \n",
    "    render_images = []\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    image_array = env.render()\n",
    "    render_images.append(PIL.Image.fromarray(image_array))\n",
    "\n",
    "    terminated, truncated = False, False\n",
    "    agent.Policy = agent.Policy.to('cpu')\n",
    "    \n",
    "    # episode start\n",
    "    while not terminated and not truncated:\n",
    "        action, log_prob = agent.get_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        image_array = env.render()\n",
    "        render_images.append(PIL.Image.fromarray(image_array))\n",
    "        \n",
    "    # episode finished\n",
    "    filename = 'play_' + name + '.gif'\n",
    "\n",
    "    # create and save GIF\n",
    "    render_images[0].save(filename, save_all=True, optimize=False, append_images=render_images[1:], duration=500, loop=0)\n",
    "\n",
    "    print(f'Episode Length : {len(render_images)-1}')\n",
    "    print(f'Total rewards : {total_reward}')\n",
    "    print('GIF is made successfully!')\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825962e",
   "metadata": {},
   "source": [
    "PIL.Image.fromarray(image_array) 는 NumPy 배열 (image_array)을 PIL 이미지 객체로 변환하는 함수/\n",
    "NumPy 배열 형태로 저장된 이미지 데이터를 PIL 라이브러리에서 다룰 수 있도록 바꿔주는 역할/\n",
    "*   입력 데이터 image_array 는 NumPy 배열이여야 하며, 배열의 차원과 데이터 유형이 중요\n",
    "*   배열의 차원: 흑백 이미지(height, width)/ 컬러 이미지(RGB) (height, width, 3)\n",
    "*   데이터 유형: 보통 uint8(0~255) 범위의 정수 값 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c646bc9",
   "metadata": {},
   "source": [
    "render_images[0]는 에피소드 시작 시 env.reset()이후 첫 번째 프레임 이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5e3a3",
   "metadata": {},
   "source": [
    "## Objective funtion of `REINFORCE`\n",
    "> $\\nabla_{\\theta} J(\\theta) = \\, \\mathbb{E}_{\\pi_{\\theta}} \\big[\\,r(\\tau)\n",
    "\\sum_{t=0}^{T-1} \\, \\nabla_{\\theta} log \\pi_{\\theta}(a_t|s_t) \\,\\big]\n",
    "\\approx \\, \\frac{1}{M}\\sum_{i=1}^M \\big[\n",
    "\\sum_{t=0}^{T-1} G_t^{(i)}\\, \\nabla_{\\theta} log \\pi_{\\theta}(a_t^{(i)}|s_t^{(i)}) \\,\\big]$\n",
    " <br>\n",
    " where $M$ is the number of episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e482a44",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "* policy를 직접 학습\n",
    "* Monte Carlo\n",
    "* stochastic\n",
    "\n",
    "* 장점: continuous action space에서 사용 가능, exploration 가능\n",
    "* 단점: convergence 느리고 variance 높음-> 학습 효율성이 낮아서 샘플 효율성 개선하기 위한 방법 필요 (ex. baseline 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e409f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_net(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_high):\n",
    "        super().__init__()\n",
    "        self.action_high = torch.FloatTensor(action_high)\n",
    "        hidden_space1 = 16\n",
    "        hidden_space2 = 32\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_space1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.ReLU() )\n",
    "        \n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_dim),\n",
    "            nn.Tanh())\n",
    "        \n",
    "        self.policy_std_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_dim) )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_mean = self.policy_mean_net(shared_features)\n",
    "        action_std = torch.log(  1 + torch.exp(self.policy_std_net(shared_features))  )\n",
    "\n",
    "        return self.action_high*action_mean, action_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4b369",
   "metadata": {},
   "source": [
    "#### action_high?\n",
    "* action 값의 최대 크기 설정\n",
    "* policy network의 출력 action_mean을 실제 환경에서 사용할 수 있는 행동 범위에 맞게 조정하기 위해 사용\n",
    "* 강화학습에서는 action 값이 환경에서 허용된 범위에 있어야 함. \n",
    "* 예를 들어 continuous action space에서 특정 변수(ex. 속도, 각도)가 물리적으로 제한 될 수 있음\n",
    "* tanh 함수가 출력값을 [-1,1]로 정규화하므로, 이를 실제 행동 범위에 맞추기 위해 action_high를 곱함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_net(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        hidden_space1 = 16\n",
    "        hidden_space2 = 32\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_space1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer= nn.Linear(hidden_space2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.shared_net(x.float())\n",
    "        state_value = self.output_layer(x)\n",
    "\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45edf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that will be interact with environment and trained\n",
    "class REINFORCE:\n",
    "    def __init__(self, state_dim, action_dim,action_high, gamma):\n",
    "        self.Policy = Policy_net(state_dim, action_dim, action_high)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.state_dim  =  state_dim\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    # get action from the Epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        action_mean, action_std = self.Policy( torch.tensor(np.array([state]))) \n",
    "        distrib = Normal(action_mean[0], action_std[0] + 1e-8) # create normal distribution\n",
    "        action = distrib.sample() # sample action from the distribution\n",
    "        log_prob = distrib.log_prob(action)   # save the log probability of the sampled action\n",
    "\n",
    "        return action.detach().numpy(), log_prob\n",
    "    \n",
    "    # update policy network\n",
    "    def update(self, optimizer, rewards, log_probs):\n",
    "        G = 0\n",
    "        loss = 0\n",
    "        for i in range(len(rewards)-1, -1, -1):\n",
    "            G = rewards[i] + self.gamma * G\n",
    "            loss += (-1) * G * log_probs[i].mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "     \n",
    "    # train agent\n",
    "    def train(self, env, max_episode, evaluate_period, evaluate_num, initial_lr, final_lr):\n",
    "        \n",
    "        start= time.time()\n",
    "        reward_list = []\n",
    "        optimizer = torch.optim.Adam(self.Policy.parameters(), lr=initial_lr)\n",
    "        \n",
    "        for episode in range(max_episode):\n",
    "            # new episode start\n",
    "            done = False\n",
    "            rewards = []\n",
    "            log_probs = []\n",
    "            \n",
    "            lr      = linear_schedule(episode, max_episode//2, initial_lr, final_lr)\n",
    "            optimizer.learning_rate = lr\n",
    "            \n",
    "            state, info = env.reset()\n",
    "            \n",
    "            # interact with environment and train\n",
    "            while not done:\n",
    "                action, log_prob = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                \n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "            # episode finished\n",
    "            # update policy\n",
    "            self.update(optimizer, rewards, log_probs)\n",
    "            # evaluate the current policy\n",
    "            if (episode+1)%evaluate_period == 0 :\n",
    "                reward = self.test(env, evaluate_num)\n",
    "                reward_list.append(reward)\n",
    "        \n",
    "        end =time.time()\n",
    "        print(f'Training time : {(end-start)/60:.2f}(min)')\n",
    "\n",
    "        return reward_list\n",
    "    \n",
    "    # evaluate current policy\n",
    "    # return average reward value over the several episodes\n",
    "    def test(self, env, evaluate_num=10):\n",
    "\n",
    "        reward_list = []\n",
    "\n",
    "        for episode in range(evaluate_num):\n",
    "            # new episode start\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            state, info = env.reset()\n",
    "            while not done:\n",
    "                action, _ = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            # episode finished\n",
    "            reward_list.append(episode_reward)\n",
    "\n",
    "        return np.mean(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that will be interact with environment and trained\n",
    "class REINFORCE_with_baseline:\n",
    "    def __init__(self, state_dim, action_dim,action_high, gamma):\n",
    "        self.Policy = Policy_net(state_dim, action_dim, action_high)\n",
    "        self.Baseline = Baseline_net(state_dim)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.state_dim  =  state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    # get action from the Epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        action_mean, action_std = self.Policy( torch.tensor(np.array([state]))) \n",
    "        distrib = Normal(action_mean[0], action_std[0] + 1e-8) # create normal distribution\n",
    "        action = distrib.sample() # sample action from the distribution\n",
    "        log_prob = distrib.log_prob(action)   # save the log probability of the sampled action\n",
    "\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    # update policy network and baseline\n",
    "    def update(self, policy_optimizer, baseline_optimizer, rewards, baseline, log_probs):\n",
    "        n = len(rewards)\n",
    "        \n",
    "        G = 0\n",
    "        policy_loss = 0\n",
    "        baseline_loss = 0\n",
    "        for i in range(n-1, -1, -1):\n",
    "            G = rewards[i] + self.gamma * G\n",
    "            policy_loss   += (-1) * (G - baseline[i].detach()) * log_probs[i].mean()\n",
    "            baseline_loss += (G - baseline[i])**2\n",
    "        \n",
    "        policy_loss  /= n\n",
    "        baseline_loss /= n\n",
    "        \n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        baseline_optimizer.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_optimizer.step()\n",
    "    \n",
    "    # train agent\n",
    "    def train(self, env, max_episode, evaluate_period, evaluate_num,\n",
    "              policy_initial_lr, policy_final_lr, baseline_initial_lr, baseline_final_lr):\n",
    "        start= time.time()\n",
    "        reward_list = []\n",
    "        policy_optimizer = torch.optim.Adam(self.Policy.parameters(), lr=policy_initial_lr)\n",
    "        baseline_optimizer = torch.optim.Adam(self.Baseline.parameters(), lr=baseline_initial_lr)\n",
    "        \n",
    "        for episode in range(max_episode):\n",
    "            # new episode start\n",
    "            done = False\n",
    "            rewards = []\n",
    "            baseline_values = []\n",
    "            log_probs = []\n",
    "            \n",
    "            policy_lr      = linear_schedule(episode, max_episode//2, policy_initial_lr, policy_final_lr)\n",
    "            policy_optimizer.learning_rate = policy_lr\n",
    "            baseline_lr      = linear_schedule(episode, max_episode//2, baseline_initial_lr, baseline_final_lr)\n",
    "            baseline_optimizer.learning_rate = baseline_lr\n",
    "            \n",
    "            state, info = env.reset()\n",
    "            \n",
    "            # interact with environment and train\n",
    "            while not done:\n",
    "                action, log_prob = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                \n",
    "                baseline_value = self.Baseline(torch.FloatTensor(state))\n",
    "                baseline_values.append(baseline_value)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                \n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "            # episode finished\n",
    "            # update\n",
    "            self.update(policy_optimizer, baseline_optimizer, rewards, baseline_values, log_probs)\n",
    "            # evaluate the current policy\n",
    "            if (episode+1)%evaluate_period == 0 :\n",
    "                reward = self.test(env, evaluate_num)\n",
    "                reward_list.append(reward)\n",
    "        \n",
    "        end =time.time()\n",
    "        print(f'Training time : {(end-start)/60:.2f}(min)')\n",
    "\n",
    "        return reward_list\n",
    "    \n",
    "    # evaluate current policy\n",
    "    # return average reward value over the several episodes\n",
    "    def test(self, env, evaluate_num=10):\n",
    "\n",
    "        reward_list = []\n",
    "\n",
    "        for episode in range(evaluate_num):\n",
    "            # new episode start\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            state, info = env.reset()\n",
    "            while not done:\n",
    "                action, _ = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            # episode finished\n",
    "            reward_list.append(episode_reward)\n",
    "\n",
    "        return np.mean(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4114fe5",
   "metadata": {},
   "source": [
    "## **Train** Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter for REINFORCE\n",
    "max_episode = 5000      # the number of episodes that agent will be trained\n",
    "evaluate_period = 5    # episode period that agent's policy will be evaluated\n",
    "evaluate_num    = 10   # the number of episodes that agent will be evaluated\n",
    "\n",
    "initial_lr = 1e-3   # starting learning rate\n",
    "final_lr   = 1e-5    # final learning rate\n",
    "gamma = 0.99    # gamma : reward discount rate\n",
    "\n",
    "repeat = 3 # repeat same experiment for the reliable result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da956375",
   "metadata": {},
   "source": [
    "* max_episode : 1000, 3000, 5000\n",
    "* initial_lr: 1e-2∼1e-3.\n",
    "* final_lr: 1e-4∼1e-5\n",
    "* gamma: 0.95∼0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : REINFORCE\n",
    "REINFORE_reward_list =[]\n",
    "for i in range(repeat):\n",
    "    # control randomness for reproducibility\n",
    "    seed = 100*(i+1)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    env = gym.make('InvertedPendulum-v5', render_mode='rgb_array')\n",
    "    agent = REINFORCE(env.observation_space.shape[0], env.action_space.shape[0],env.action_space.high, gamma)\n",
    "    reward = agent.train( env, max_episode, evaluate_period, evaluate_num, initial_lr, final_lr )\n",
    "    REINFORE_reward_list.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12179c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter for REINFORCE with baseline\n",
    "max_episode = 5000      # the number of episodes that agent will be trained\n",
    "evaluate_period = 5    # episode period that agent's policy will be evaluated\n",
    "evaluate_num    = 10   # the number of episodes that agent will be evaluated\n",
    "\n",
    "policy_initial_lr = 1e-3\n",
    "policy_final_lr   = 1e-5\n",
    "baseline_initial_lr = 1e-3\n",
    "baseline_final_lr   = 1e-5\n",
    "gamma = 0.99\n",
    "\n",
    "repeat = 3 # repeat same experiment for the reliable result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b17a7",
   "metadata": {},
   "source": [
    "* max_episode : 1000, 3000, 5000\n",
    "* policy_initial_lr = 1e-2∼1e-3\n",
    "* policy_final_lr   = 1e-4∼1e-5\n",
    "* baseline_initial_lr = 1e-2∼1e-3\n",
    "* baseline_final_lr   = 1e-4∼1e-5\n",
    "* gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : REINFORCE with baseline\n",
    "REINFORE_with_baseline_reward_list =[]\n",
    "for i in range(repeat):\n",
    "    # control randomness for reproducibility\n",
    "    seed = 100*(i+1)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    env = gym.make('InvertedPendulum-v5', render_mode='rgb_array')\n",
    "    agent = REINFORCE_with_baseline(env.observation_space.shape[0], env.action_space.shape[0],env.action_space.high, gamma)\n",
    "    reward = agent.train( env, max_episode, evaluate_period, evaluate_num, \n",
    "                         policy_initial_lr, policy_final_lr, baseline_initial_lr, baseline_final_lr )\n",
    "    REINFORE_with_baseline_reward_list.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931002c",
   "metadata": {},
   "source": [
    "## **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2c1fa",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save the result\n",
    "save_folder = '/home/work/DLmath/Seulbin/DRLstudy/20241120_REINFORCE/result/'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'REINFORCE9.png'\n",
    "save_path  = os.path.join(save_folder, save_file)\n",
    "\n",
    "plot(REINFORE_reward_list, 'REINFORCE', save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'REINFORCE_with_baseline9.png'\n",
    "save_path  = os.path.join(save_folder, save_file)\n",
    "\n",
    "plot(REINFORE_with_baseline_reward_list, 'REINFORCE with baseline', save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cc9c1",
   "metadata": {},
   "source": [
    "### Test the trained agent and save it into a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''display = Display(visible=0, size=(140, 90))\n",
    "display.start()\n",
    "\n",
    "env = gym.make('InvertedPendulum-v5', render_mode='rgb_array')\n",
    "play = play_and_save(env, agent, 'REINFORCE', seed=8)\n",
    "\n",
    "display.stop()\n",
    "Image(open(play,'rb').read())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f67512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
